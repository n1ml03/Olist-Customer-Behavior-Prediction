{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center>Reviews Sentiment Analysis</center></h1>\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqlY-Afla578"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "file_id = '1_8Zdnq5Jo_yakab3kuQ0GPzpVu5nuMph'\n",
        "output_path = 'olist_order_reviews_dataset.csv'\n",
        "gdown.download(id=file_id, output=output_path, quiet=True, fuzzy=True)"
      ],
      "metadata": {
        "id": "79unaRXCw0mi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "8e9f5b64-8142-4326-c2a0-d75cb486e645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'olist_order_reviews_dataset.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required library\n",
        "!pip install -q shap\n",
        "\n",
        "# Standard libs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from matplotlib.gridspec import GridSpec\n",
        "pd.set_option('display.max_columns', 100)\n",
        "import plotly.offline as py\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import json\n",
        "import requests\n",
        "import folium\n",
        "from folium.plugins import FastMarkerCluster, Fullscreen, MiniMap, HeatMap, HeatMapWithTime, LocateControl\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Utilities\n",
        "sys.path.append('/content/sentiment/')\n",
        "from viz_utils.py import *\n",
        "from custom_transformers import *\n",
        "from ml_utils import *\n",
        "\n",
        "# DataPrep\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "\n",
        "# Modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import lightgbm as lgb"
      ],
      "metadata": {
        "id": "oVelfJRUxKiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Understanding"
      ],
      "metadata": {
        "id": "YGm3pS6Uv3UE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "olist_order_reviews = pd.read_csv('olist_order_reviews_dataset.csv')"
      ],
      "metadata": {
        "id": "QyEdSJf_xuQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_comments = olist_order_reviews.loc[:, ['review_score', 'review_comment_message']]\n",
        "df_comments = df_comments.dropna(subset=['review_comment_message'])\n",
        "df_comments = df_comments.reset_index(drop=True)\n",
        "print(f'Dataset shape: {df_comments.shape}')\n",
        "df_comments.columns = ['score', 'comment']\n",
        "df_comments"
      ],
      "metadata": {
        "id": "rSU80zKlxY1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular Expressions"
      ],
      "metadata": {
        "id": "i_Mi10kPwEXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_patterns(re_pattern, text_list):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ---------\n",
        "    re_pattern: regular expression pattern to be used on search [type: string]\n",
        "    text_list: list with text strings [type: list]\n",
        "\n",
        "    Returns:\n",
        "    positions_dict: python dictionary with key-value pars as below:\n",
        "        text_idx: [(start_pattern1, end_pattern1), (start_pattern1, end_pattern2), ... (start_n, end_n)]\n",
        "    \"\"\"\n",
        "\n",
        "    # Compiling the Regular Expression passed as a arg\n",
        "    p = re.compile(re_pattern)\n",
        "    positions_dict = {}\n",
        "    i = 0\n",
        "    for c in text_list:\n",
        "        match_list = []\n",
        "        iterator = p.finditer(c)\n",
        "        for match in iterator:\n",
        "            match_list.concat(match.span())\n",
        "        control_key = f'Text idx {i}'\n",
        "        if len(match_list) == 0:\n",
        "            pass\n",
        "        else:\n",
        "            positions_dict[control_key] = match_list\n",
        "        i += 1\n",
        "\n",
        "    \"\"\"p = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n",
        "    pattern_dict = find_patterns(p, reviews_breakline)\n",
        "    print(len(pattern_dict))\n",
        "    pattern_dict\n",
        "    for idx in [int(c.split(' ')[-1]) for c in list(pattern_dict.keys())]:\n",
        "        print(f'{reviews_breakline[idx]}\\n')\"\"\"\n",
        "\n",
        "    return positions_dict\n",
        "\n",
        "def print_step_result(text_list_before, text_list_after, idx_list):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text_list_before: list object with text content before transformation [type: list]\n",
        "    text_list_after: list object with text content after transformation [type: list]\n",
        "    idx_list: list object with indexes to be printed [type: list]\n",
        "    \"\"\"\n",
        "\n",
        "    # Iterating over string examples\n",
        "    i = 1\n",
        "    for idx in idx_list:\n",
        "        print(f'--- Text {i} ---\\n')\n",
        "        print(f'Before: \\n{text_list_before[idx]}\\n')\n",
        "        print(f'After: \\n{text_list_after[idx]}\\n')\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "LI5nwJ27x3z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Breakline and Carriage Return"
      ],
      "metadata": {
        "id": "MPrf4iM9wh4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def re_breakline(text_list):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text_list: list object with text content to be prepared [type: list]\n",
        "    \"\"\"\n",
        "\n",
        "    # Applying regex\n",
        "    return [re.sub('[\\n\\r]', ' ', r) for r in text_list]"
      ],
      "metadata": {
        "id": "9igVg--Hx5ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of comment reviews\n",
        "reviews = list(df_comments['comment'].values)\n",
        "\n",
        "# Applying RegEx\n",
        "reviews_breakline = re_breakline(reviews)\n",
        "df_comments['re_breakline'] = reviews_breakline\n",
        "\n",
        "# Verifying results\n",
        "print_step_result(reviews, reviews_breakline, idx_list=[48])"
      ],
      "metadata": {
        "id": "e0ILjpqFx7Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sites and Hiperlinks"
      ],
      "metadata": {
        "id": "HZ2mrW-bwnSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def re_hiperlinks(text_list):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text_list: list object with text content to be prepared [type: list]\n",
        "    \"\"\"\n",
        "\n",
        "    # Applying regex\n",
        "    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    return [re.sub(pattern, ' link ', r) for r in text_list]"
      ],
      "metadata": {
        "id": "JILMIEbHx-cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying RegEx\n",
        "reviews_hiperlinks = re_hiperlinks(reviews_breakline)\n",
        "df_comments['re_hiperlinks'] = reviews_hiperlinks\n",
        "\n",
        "# Verifying results\n",
        "print_step_result(reviews_breakline, reviews_hiperlinks, idx_list=[10796, 12782])"
      ],
      "metadata": {
        "id": "GMKHd_Fmx_ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dates"
      ],
      "metadata": {
        "id": "LKUL4ngSwq3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def re_dates(text_list):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text_list: list object with text content to be prepared [type: list]\n",
        "    \"\"\"\n",
        "\n",
        "    # Applying regex\n",
        "    pattern = '([0-2][0-9]|(3)[0-1])(\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\/|\\.)\\d{2,4}'\n",
        "    return [re.sub(pattern, ' data ', r) for r in text_list]"
      ],
      "metadata": {
        "id": "Sm11HxACyBN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying RegEx\n",
        "reviews_dates = re_dates(reviews_hiperlinks)\n",
        "df_comments['re_dates'] = reviews_dates\n",
        "\n",
        "# Verifying results\n",
        "print_step_result(reviews_hiperlinks, reviews_dates, idx_list=[26665, 40710, 40887])"
      ],
      "metadata": {
        "id": "jQie7dCEyCkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Money"
      ],
      "metadata": {
        "id": "KCXJwMnowt-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def re_money(text_list):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text_list: list object with text content to be prepared [type: list]\n",
        "    \"\"\"\n",
        "\n",
        "    # Applying regex\n",
        "    pattern = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n",
        "    return [re.sub(pattern, ' money ', r) for r in text_list]"
      ],
      "metadata": {
        "id": "QggHlE5IyELf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying RegEx\n",
        "reviews_money = re_money(reviews_dates)\n",
        "df_comments['re_money'] = reviews_money\n",
        "\n",
        "# Verifying results\n",
        "print_step_result(reviews_dates, reviews_money, idx_list=[26020, 33297, 32998])"
      ],
      "metadata": {
        "id": "IwV0DFG8yFdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numbers"
      ],
      "metadata": {
        "id": "ghH6wLSrwxOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def re_numbers(text_list):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text_series: list object with text content to be prepared [type: list]\n",
        "    \"\"\"\n",
        "\n",
        "    # Applying regex\n",
        "    return [re.sub('[0-9]+', ' number ', r) for r in text_list]"
      ],
      "metadata": {
        "id": "9NhoJsuqyG8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying RegEx\n",
        "reviews_numbers = re_numbers(reviews_money)\n",
        "df_comments['re_numbers'] = reviews_numbers\n",
        "\n",
        "# Verifying results\n",
        "print_step_result(reviews_money, reviews_numbers, idx_list=[68])"
      ],
      "metadata": {
        "id": "Nht5z7_xyIbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Negation"
      ],
      "metadata": {
        "id": "CUee96cCw0IU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def re_negation(text_list):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text_series: list object with text content to be prepared [type: list]\n",
        "    \"\"\"\n",
        "\n",
        "    # Applying regex\n",
        "    return [re.sub('([nN][ãÃaA][oO]|[ñÑ]| [nN] )', ' negation ', r) for r in text_list]"
      ],
      "metadata": {
        "id": "RkmbdzfpyJ1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying RegEx\n",
        "reviews_negation = re_negation(reviews_numbers)\n",
        "df_comments['re_negation'] = reviews_negation\n",
        "\n",
        "# Verifying results\n",
        "print_step_result(reviews_numbers, reviews_negation, idx_list=[38, 247, 1396, 2401])"
      ],
      "metadata": {
        "id": "4SWwGLWtyLRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Special Characters"
      ],
      "metadata": {
        "id": "YKbyaxuDw2_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def re_special_chars(text_list):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text_series: list object with text content to be prepared [type: list]\n",
        "    \"\"\"\n",
        "\n",
        "    # Applying regex\n",
        "    return [re.sub('\\W', ' ', r) for r in text_list]"
      ],
      "metadata": {
        "id": "ZFduMUp6yMzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying RegEx\n",
        "reviews_special_chars = re_special_chars(reviews_negation)\n",
        "df_comments['re_special_chars'] = reviews_special_chars\n",
        "\n",
        "# Verifying results\n",
        "print_step_result(reviews_negation, reviews_special_chars, idx_list=[45, 135, 234])"
      ],
      "metadata": {
        "id": "hIHf8EdXyOFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Whitespaces"
      ],
      "metadata": {
        "id": "o_-LrhdCw58O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def re_whitespaces(text_list):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text_series: list object with text content to be prepared [type: list]\n",
        "    \"\"\"\n",
        "\n",
        "    # Applying regex\n",
        "    white_spaces = [re.sub('\\s+', ' ', r) for r in text_list]\n",
        "    white_spaces_end = [re.sub('[ \\t]+$', '', r) for r in white_spaces]\n",
        "    return white_spaces_end"
      ],
      "metadata": {
        "id": "W16dYPLIyP7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying RegEx\n",
        "reviews_whitespaces = re_whitespaces(reviews_special_chars)\n",
        "df_comments['re_whitespaces'] = reviews_whitespaces\n",
        "\n",
        "# Verifying results\n",
        "print_step_result(reviews_special_chars, reviews_whitespaces, idx_list=[3, 4, -1])"
      ],
      "metadata": {
        "id": "eYbQxd9oyRQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords"
      ],
      "metadata": {
        "id": "0U9lvvu0w-MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "4GQYVxVi3LB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples of some english stopwords\n",
        "pt_stopwords = stopwords.words('english')\n",
        "print(f'Total enlgish stopwords in the nltk.corpous module: {len(pt_stopwords)}')\n",
        "pt_stopwords[:10]"
      ],
      "metadata": {
        "id": "gV44YNofyS74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to remove the stopwords and to lower the comments\n",
        "def stopwords_removal(text, cached_stopwords=stopwords.words('english')):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text: list object where the stopwords will be removed [type: list]\n",
        "    cached_stopwords: stopwords to be applied on the process [type: list, default: stopwords.words('english')]\n",
        "    \"\"\"\n",
        "\n",
        "    return [c.lower() for c in text.split() if c.lower() not in cached_stopwords]"
      ],
      "metadata": {
        "id": "f9kcQ9umyUEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords and looking at some examples\n",
        "reviews_stopwords = [' '.join(stopwords_removal(review)) for review in reviews_whitespaces]\n",
        "df_comments['stopwords_removed'] = reviews_stopwords\n",
        "\n",
        "print_step_result(reviews_whitespaces, reviews_stopwords, idx_list=[0, 28, 500])"
      ],
      "metadata": {
        "id": "VR9xcKqgyYFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "PTYee05zxFbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('rslp')"
      ],
      "metadata": {
        "id": "FkOABtau3iOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to remove the stopwords and to lower the comments\n",
        "def stemming_process(text, stemmer=RSLPStemmer()):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    ----------\n",
        "    text: list object where the stopwords will be removed [type: list]\n",
        "    stemmer: type of stemmer to be applied [type: class, default: RSLPStemmer()]\n",
        "    \"\"\"\n",
        "\n",
        "    return [stemmer.stem(c) for c in text.split()]"
      ],
      "metadata": {
        "id": "BTPz9eJ6yZ0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying stemming and looking at some examples\n",
        "reviews_stemmer = [' '.join(stemming_process(review)) for review in reviews_stopwords]\n",
        "df_comments['stemming'] = reviews_stemmer\n",
        "\n",
        "print_step_result(reviews_stopwords, reviews_stemmer, idx_list=[0, 45, -1])"
      ],
      "metadata": {
        "id": "a6B0fzYWybHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction"
      ],
      "metadata": {
        "id": "68sYMffFxQwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_from_corpus(corpus, vectorizer, df=False):\n",
        "    \"\"\"\n",
        "    Args\n",
        "    ------------\n",
        "    text: text to be transformed into a document-term matrix [type: string]\n",
        "    vectorizer: engine to be used in the transformation [type: object]\n",
        "    \"\"\"\n",
        "\n",
        "    # Extracting features\n",
        "    corpus_features = vectorizer.fit_transform(corpus).toarray()\n",
        "    features_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Transforming into a dataframe to give interpetability to the process\n",
        "    df_corpus_features = None\n",
        "    if df:\n",
        "        df_corpus_features = pd.DataFrame(corpus_features, columns=features_names)\n",
        "\n",
        "    return corpus_features, df_corpus_features"
      ],
      "metadata": {
        "id": "SdKigbeXyc39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CountVectorizer"
      ],
      "metadata": {
        "id": "bPaRUyALxVlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an object for the CountVectorizer class\n",
        "count_vectorizer = CountVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=pt_stopwords)\n",
        "\n",
        "# Extracting features for the corpus\n",
        "countv_features, df_countv_features = extract_features_from_corpus(reviews_stemmer, count_vectorizer, df=True)\n",
        "print(f'Shape of countv_features matrix: {countv_features.shape}\\n')\n",
        "print(f'Example of DataFrame of corpus features:')\n",
        "df_countv_features.head()"
      ],
      "metadata": {
        "id": "O_7Ce_T5yeSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "0HtPcICbxZEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an object for the CountVectorizer class\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=pt_stopwords)\n",
        "\n",
        "# Extracting features for the corpus\n",
        "tfidf_features, df_tfidf_features = extract_features_from_corpus(reviews_stemmer, tfidf_vectorizer, df=True)\n",
        "print(f'Shape of tfidf_features matrix: {tfidf_features.shape}\\n')\n",
        "print(f'Example of DataFrame of corpus features:')\n",
        "df_tfidf_features.head()"
      ],
      "metadata": {
        "id": "6KiYq3m-ygkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labelling Data"
      ],
      "metadata": {
        "id": "8aoW_yeixgAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "single_countplot(x='score', df=df_comments, ax=ax)"
      ],
      "metadata": {
        "id": "d0H9rrEIyioL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labelling data\n",
        "score_map = {\n",
        "    1: 'negative',\n",
        "    2: 'negative',\n",
        "    3: 'positive',\n",
        "    4: 'positive',\n",
        "    5: 'positive'\n",
        "}\n",
        "df_comments['sentiment_label'] = df_comments['score'].map(score_map)\n",
        "\n",
        "# Verifying results\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "donut_plot(df_comments.query('sentiment_label in (\"positive\", \"negative\")'), 'sentiment_label',\n",
        "           label_names=df_comments.query('sentiment_label in (\"positive\", \"negative\")')['sentiment_label'].value_counts().index,\n",
        "           ax=ax, colors=['darkslateblue', 'crimson'])"
      ],
      "metadata": {
        "id": "FQYbZ9yTykG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrams_count(corpus, ngram_range, n=-1, cached_stopwords=stopwords.words('english')):\n",
        "    \"\"\"\n",
        "    Args\n",
        "    ----------\n",
        "    corpus: text to be analysed [type: pd.DataFrame]\n",
        "    ngram_range: type of n gram to be used on analysis [type: tuple]\n",
        "    n: top limit of ngrams to be shown [type: int, default: -1]\n",
        "    \"\"\"\n",
        "\n",
        "    # Using CountVectorizer to build a bag of words using the given corpus\n",
        "    vectorizer = CountVectorizer(stop_words=cached_stopwords, ngram_range=ngram_range).fit(corpus)\n",
        "    bag_of_words = vectorizer.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    total_list = words_freq[:n]\n",
        "\n",
        "    # Returning a DataFrame with the ngrams count\n",
        "    count_df = pd.DataFrame(total_list, columns=['ngram', 'count'])\n",
        "    return count_df"
      ],
      "metadata": {
        "id": "Q4qglNVaynMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the corpus into positive and negative comments\n",
        "positive_comments = df_comments.query('sentiment_label == \"positive\"')['stemming']\n",
        "negative_comments = df_comments.query('sentiment_label == \"negative\"')['stemming']\n",
        "\n",
        "# Extracting the top 10 unigrams by sentiment\n",
        "unigrams_pos = ngrams_count(positive_comments, (1, 1), 10)\n",
        "unigrams_neg = ngrams_count(negative_comments, (1, 1), 10)\n",
        "\n",
        "# Extracting the top 10 unigrams by sentiment\n",
        "bigrams_pos = ngrams_count(positive_comments, (2, 2), 10)\n",
        "bigrams_neg = ngrams_count(negative_comments, (2, 2), 10)\n",
        "\n",
        "# Extracting the top 10 unigrams by sentiment\n",
        "trigrams_pos = ngrams_count(positive_comments, (3, 3), 10)\n",
        "trigrams_neg = ngrams_count(negative_comments, (3, 3), 10)"
      ],
      "metadata": {
        "id": "2aIwhCZqyoas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining everything in a python dictionary to make the plots easier\n",
        "ngram_dict_plot = {\n",
        "    'Top Unigrams on Positive Comments': unigrams_pos,\n",
        "    'Top Unigrams on Negative Comments': unigrams_neg,\n",
        "    'Top Bigrams on Positive Comments': bigrams_pos,\n",
        "    'Top Bigrams on Negative Comments': bigrams_neg,\n",
        "    'Top Trigrams on Positive Comments': trigrams_pos,\n",
        "    'Top Trigrams on Negative Comments': trigrams_neg,\n",
        "}\n",
        "\n",
        "# Plotting the ngrams analysis\n",
        "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 18))\n",
        "i, j = 0, 0\n",
        "colors = ['Blues_d', 'Reds_d']\n",
        "for title, ngram_data in ngram_dict_plot.items():\n",
        "    ax = axs[i, j]\n",
        "    sns.barplot(x='count', y='ngram', data=ngram_data, ax=ax, palette=colors[j])\n",
        "\n",
        "    # Customizing plots\n",
        "    format_spines(ax, right_border=False)\n",
        "    ax.set_title(title, size=14)\n",
        "    ax.set_ylabel('')\n",
        "    ax.set_xlabel('')\n",
        "\n",
        "    # Incrementing the index\n",
        "    j += 1\n",
        "    if j == 2:\n",
        "        j = 0\n",
        "        i += 1\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gcCZ0N3Dyp2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "UE-Pl0aPxoIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for regular expressions application\n",
        "class ApplyRegex(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, regex_transformers):\n",
        "        self.regex_transformers = regex_transformers\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        # Applying all regex functions in the regex_transformers dictionary\n",
        "        for regex_name, regex_function in self.regex_transformers.items():\n",
        "            X = regex_function(X)\n",
        "\n",
        "        return X\n",
        "\n",
        "# Class for stopwords removal from the corpus\n",
        "class StopWordsRemoval(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, text_stopwords):\n",
        "        self.text_stopwords = text_stopwords\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return [' '.join(stopwords_removal(comment, self.text_stopwords)) for comment in X]\n",
        "\n",
        "# Class for apply the stemming process\n",
        "class StemmingProcess(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, stemmer):\n",
        "        self.stemmer = stemmer\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return [' '.join(stemming_process(comment, self.stemmer)) for comment in X]\n",
        "\n",
        "# Class for extracting features from corpus\n",
        "class TextFeatureExtraction(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, vectorizer):\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return self.vectorizer.fit_transform(X).toarray()"
      ],
      "metadata": {
        "id": "kTWp_lTgyr94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining regex transformers to be applied\n",
        "regex_transformers = {\n",
        "    'break_line': re_breakline,\n",
        "    'hiperlinks': re_hiperlinks,\n",
        "    'dates': re_dates,\n",
        "    'money': re_money,\n",
        "    'numbers': re_numbers,\n",
        "    'negation': re_negation,\n",
        "    'special_chars': re_special_chars,\n",
        "    'whitespaces': re_whitespaces\n",
        "}\n",
        "\n",
        "# Defining the vectorizer to extract features from text\n",
        "vectorizer = TfidfVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=pt_stopwords)\n",
        "\n",
        "# Building the Pipeline\n",
        "text_pipeline = Pipeline([\n",
        "    ('regex', ApplyRegex(regex_transformers)),\n",
        "    ('stopwords', StopWordsRemoval(stopwords.words('english'))),\n",
        "    ('stemming', StemmingProcess(RSLPStemmer())),\n",
        "    ('text_features', TextFeatureExtraction(vectorizer))\n",
        "])"
      ],
      "metadata": {
        "id": "VmfaciiUytr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining X and y\n",
        "idx_reviews = olist_order_reviews['review_comment_message'].dropna().index\n",
        "score = olist_order_reviews['review_score'][idx_reviews].map(score_map)\n",
        "\n",
        "# Splitting into train and test sets\n",
        "X = list(olist_order_reviews['review_comment_message'][idx_reviews].values)\n",
        "y = score.apply(lambda x: 1 if x == 'positive' else 0).values\n",
        "\n",
        "# Applying the pipeline and splitting the data\n",
        "X_processed = text_pipeline.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=.20, random_state=42)\n",
        "\n",
        "# Verifying results\n",
        "print(f'Length of X_train_processed: {len(X_train)} - Length of one element: {len(X_train[0])}')\n",
        "print(f'Length of X_test_processed: {len(X_test)} - Length of one element: {len(X_test[0])}')"
      ],
      "metadata": {
        "id": "qykVxYdvyvgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Classification"
      ],
      "metadata": {
        "id": "2MbKRkfkxvlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression hyperparameters\n",
        "logreg_param_grid = {\n",
        "    'C': np.linspace(0.1, 10, 20),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'class_weight': ['balanced', None],\n",
        "    'random_state': [42],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "# Setting up the classifiers\n",
        "set_classifiers = {\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(),\n",
        "        'params': logreg_param_grid\n",
        "    },\n",
        "    'Naive Bayes': {\n",
        "        'model': GaussianNB(),\n",
        "        'params': {}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "S_F2wqzny0hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an object and training the classifiers\n",
        "clf_tool = BinaryClassifiersAnalysis()\n",
        "clf_tool.fit(set_classifiers, X_train, y_train, random_search=True, scoring='accuracy')"
      ],
      "metadata": {
        "id": "NZ5-k-3Wy145"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating metrics\n",
        "df_performances = clf_tool.evaluate_performance(X_train, y_train, X_test, y_test, cv=5)\n",
        "df_performances.reset_index(drop=True).style.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "id": "sdn61XCyy3SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_tool.plot_confusion_matrix(classes=['Negative', 'Positive'])"
      ],
      "metadata": {
        "id": "N4iiAss5y5CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to plot the sentiment of a given phrase\n",
        "def sentiment_analysis(text, pipeline, vectorizer, model):\n",
        "    \"\"\"\n",
        "    Args\n",
        "    -----------\n",
        "    text: text string / phrase / review comment to be analysed [type: string]\n",
        "    pipeline: text prep pipeline built for preparing the corpus [type: sklearn.Pipeline]\n",
        "    model: classification model trained to recognize positive and negative sentiment [type: model]\n",
        "    \"\"\"\n",
        "\n",
        "    # Applying the pipeline\n",
        "    if type(text) is not list:\n",
        "        text = [text]\n",
        "    text_prep = pipeline.fit_transform(text)\n",
        "    matrix = vectorizer.transform(text_prep)\n",
        "\n",
        "    # Predicting sentiment\n",
        "    pred = model.predict(matrix)\n",
        "    proba = model.predict_proba(matrix)\n",
        "\n",
        "    # Plotting the sentiment and its score\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "    if pred[0] == 1:\n",
        "        text = 'Positive'\n",
        "        class_proba = 100 * round(proba[0][1], 2)\n",
        "        color = 'seagreen'\n",
        "    else:\n",
        "        text = 'Negative'\n",
        "        class_proba = 100 * round(proba[0][0], 2)\n",
        "        color = 'crimson'\n",
        "    ax.text(0.5, 0.5, text, fontsize=50, ha='center', color=color)\n",
        "    ax.text(0.5, 0.20, str(class_proba) + '%', fontsize=14, ha='center')\n",
        "    ax.axis('off')\n",
        "    ax.set_title('Sentiment Analysis', fontsize=14)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "plFgvER9y6iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Implementation"
      ],
      "metadata": {
        "id": "QSQ9dj0Ox09l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining transformers for preparing the text input\n",
        "model = clf_tool.classifiers_info['LogisticRegression']['estimator']\n",
        "prod_pipeline = Pipeline([\n",
        "    ('regex', ApplyRegex(regex_transformers)),\n",
        "    ('stopwords', StopWordsRemoval(stopwords.words('english'))),\n",
        "    ('stemming', StemmingProcess(RSLPStemmer()))\n",
        "])\n",
        "vectorizer = text_pipeline.named_steps['text_features'].vectorizer"
      ],
      "metadata": {
        "id": "-XqFu4HJy8De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment = \"Terrible product! I don't buy from this store, the delivery was late and cost a lot of money!\"\n",
        "sentiment_analysis(comment, pipeline=prod_pipeline, vectorizer=vectorizer, model=model)"
      ],
      "metadata": {
        "id": "8FYoE_KJy-Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment = 'I loved it and it really lived up to expectations. I bought it for a cheap price. Wonderful'\n",
        "sentiment_analysis(comment, pipeline=prod_pipeline, vectorizer=vectorizer, model=model)"
      ],
      "metadata": {
        "id": "puUR4J0Vy_2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment = \"I don't know, I liked the product. The cost was cheap but it arrived defective. If you're lucky, it's worth it\"\n",
        "sentiment_analysis(comment, pipeline=prod_pipeline, vectorizer=vectorizer, model=model)"
      ],
      "metadata": {
        "id": "QqGau08tzBV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model sentiment reviews**"
      ],
      "metadata": {
        "id": "aAJ7RN5PKzCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from joblib import dump\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from textblob import TextBlob\n",
        "\n",
        "from custom_transformers import DropNullData, DropDuplicates\n",
        "from ml_utils import BinaryClassifiersAnalysis, cross_val_performance\n",
        "from text_utils import re_breakline, re_dates, re_hiperlinks, re_money, re_negation, re_numbers, \\\n",
        "    re_special_chars, re_whitespaces, ApplyRegex, StemmingProcess\n",
        "from sklearn.metrics import balanced_accuracy_score"
      ],
      "metadata": {
        "id": "wRXXlHS8EAQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This python script will allocate all the custom transformers that are specific for the project task.\n",
        "The idea is to encapsulate the classes and functions used on pipelines to make codes cleaner.\n",
        "\"\"\"\n",
        "\n",
        "# Importing libraries\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "----- 1. CUSTOM TRANSFORMERS ------\n",
        "           1.1 Classes\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class ColumnMapping(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    This class applies the map() function into a DataFrame for transforming a columns given a mapping dictionary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    :param old_col_name: name of the columns where mapping will be applied [type: string]\n",
        "    :param mapping_dict: python dictionary with key/value mapping [type: dict]\n",
        "    :param new_col_name: name of the new column resulted by mapping [type: string, default: 'target]\n",
        "    :param drop: flag that guides the dropping of the old_target_name column [type: bool, default: True]\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    :return X: pandas DataFrame object after mapping application [type: pd.DataFrame]\n",
        "\n",
        "    Application\n",
        "    -----------\n",
        "    # Transforming a DataFrame column given a mapping dictionary\n",
        "    mapper = ColumnMapping(old_col_name='col_1', mapping_dict=dictionary, new_col_name='col_2', drop=True)\n",
        "    df_mapped = mapper.fit_transform(df)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, old_col_name, mapping_dict, new_col_name='review_score', drop=True):\n",
        "        self.old_col_name = old_col_name\n",
        "        self.mapping_dict = mapping_dict\n",
        "        self.new_col_name = new_col_name\n",
        "        self.drop = drop\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        # Applying mpping\n",
        "        X[self.new_col_name] = X[self.old_col_name].map(self.mapping_dict)\n",
        "\n",
        "        # Dropping the old columns (if applicable)\n",
        "        if self.drop:\n",
        "            X.drop(self.old_col_name, axis=1, inplace=True)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "fbeOXQKECnDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "--- SUMMARY ---\n",
        "\n",
        "1. Project Variables\n",
        "2. Reading Data\n",
        "3. Prep Pipelines\n",
        "    3.1 Initial Preparation\n",
        "    3.2 Text Transformers\n",
        "4. Modeling\n",
        "    4.1 Model Training\n",
        "    4.2 Evaluating Metrics\n",
        "    4.3 Complete Solution\n",
        "    4.4 Final Model Performance\n",
        "    4.5 Saving pkl Files\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Importing libs\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import dump\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sentiment.custom_transformers import DropNullData, DropDuplicates\n",
        "from sentiment.ml_utils import BinaryClassifiersAnalysis, cross_val_performance\n",
        "from sentiment.text_utils import re_breakline, re_dates, re_hiperlinks, re_money, re_negation, re_numbers, \\\n",
        "    re_special_chars, re_whitespaces, ApplyRegex, StemmingProcess\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "------ 1. PROJECT VARIABLES -------\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "# Variables for address paths\n",
        "DATA_PATH = ''\n",
        "PIPELINES_PATH = '/content/pipelines' # Take a look at your project structure\n",
        "MODELS_PATH = '/content/models' # Take a look at your project structure\n",
        "\n",
        "# Variables for reading the data\n",
        "FILENAME = 'olist_master.csv'\n",
        "COLS_READ = ['product_id', 'review_comment_message', 'review_score']\n",
        "CORPUS_COL = 'review_comment_message'\n",
        "TARGET_COL = 'review_score'\n",
        "\n",
        "\n",
        "# Variables for saving data\n",
        "METRICS_FILEPATH = '/content/metrics/model_performance.csv' # Take a look at your project structure\n",
        "\n",
        "# Variables for retrieving model\n",
        "MODEL_KEY = 'LogisticRegression'\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "-------- 2. READING DATA ----------\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "# Reading the data with text corpus and score\n",
        "df = pd.read_csv(os.path.join(DATA_PATH, FILENAME), usecols=COLS_READ)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "------- 3. PREP PIPELINES ---------\n",
        "    3.1 Initial Preparation\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "# Creating a dictionary for mapping the target column based on review score\n",
        "score_map = {\n",
        "    1: 0,\n",
        "    2: 0,\n",
        "    3: 0,\n",
        "    4: 1,\n",
        "    5: 1\n",
        "}\n",
        "\n",
        "# Creating a pipeline for the initial prep on the data\n",
        "initial_prep_pipeline = Pipeline([\n",
        "    ('mapper', ColumnMapping(old_col_name='review_score', mapping_dict=score_map, new_col_name=TARGET_COL)),\n",
        "    ('null_dropper', DropNullData()),\n",
        "    ('dup_dropper', DropDuplicates())\n",
        "])\n",
        "\n",
        "# Applying initial prep pipeline\n",
        "df_prep = initial_prep_pipeline.fit_transform(df)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "------- 3. PREP PIPELINES ---------\n",
        "      3.2 Text Transformers\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "# Defining regex transformers to be applied\n",
        "regex_transformers = {\n",
        "    'break_line': re_breakline,\n",
        "    'hiperlinks': re_hiperlinks,\n",
        "    'dates': re_dates,\n",
        "    'money': re_money,\n",
        "    'numbers': re_numbers,\n",
        "    'negation': re_negation,\n",
        "    'special_chars': re_special_chars,\n",
        "    'whitespaces': re_whitespaces\n",
        "}\n",
        "\n",
        "# Building a text prep pipeline\n",
        "text_prep_pipeline = Pipeline([\n",
        "    ('regex', ApplyRegex(regex_transformers)),\n",
        "    ('stemming', StemmingProcess(SnowballStemmer('english'))),\n",
        "    ('vectorizer', TfidfVectorizer(max_features=500, min_df=7, max_df=0.8))\n",
        "])\n",
        "\n",
        "# Applying the pipeline\n",
        "X = df_prep[CORPUS_COL].tolist()\n",
        "y = df_prep[TARGET_COL].tolist()\n",
        "X_prep = text_prep_pipeline.fit_transform(X)\n",
        "\n",
        "# Splitting the data into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_prep, y, test_size=.20, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "--------- 4. MODELING  -----------\n",
        "       4.1 Model Training\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "# Specifing a Logistic Regression model for sentiment classification\n",
        "logreg_param_grid = {\n",
        "    'C': np.linspace(0.1, 10, 20),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'class_weight': ['balanced', None],\n",
        "    'random_state': [42],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "# Setting up the classifiers\n",
        "set_classifiers = {\n",
        "    'LogisticRegression': {\n",
        "        'model': LogisticRegression(),\n",
        "        'params': logreg_param_grid\n",
        "    }\n",
        "}\n",
        "\n",
        "# Creating an object and training the classifiers\n",
        "trainer = BinaryClassifiersAnalysis()\n",
        "trainer.fit(set_classifiers, X_train, y_train, random_search=True, scoring='accuracy')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "--------- 4. MODELING  -----------\n",
        "    4.2 Evaluating Metrics\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "# Evaluating metrics\n",
        "performance = trainer.evaluate_performance(X_train, y_train, X_test, y_test, cv=5, save=False, overwrite=True,\n",
        "                                           performances_filepath=METRICS_FILEPATH)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "--------- 4. MODELING  -----------\n",
        "    4.3. Complete Solution\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "# Returning the model to be saved\n",
        "model = trainer.classifiers_info[MODEL_KEY]['estimator']\n",
        "\n",
        "# Creating a complete pipeline for prep and predict\n",
        "e2e_pipeline = Pipeline([\n",
        "    ('text_prep', text_prep_pipeline),\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "# Defining a param grid for searching best pipelines options (reduced options for making the search faster)\n",
        "\"\"\"param_grid = [{\n",
        "    'text_prep__vectorizer__max_features': np.arange(500, 851, 50),\n",
        "    'text_prep__vectorizer__min_df': [7, 9, 12, 15, 30],\n",
        "    'text_prep__vectorizer__max_df': [.4, .5, .6, .7]\n",
        "}]\"\"\"\n",
        "\n",
        "param_grid = [{\n",
        "    'text_prep__vectorizer__max_features': np.arange(600, 601, 50),\n",
        "    'text_prep__vectorizer__min_df': [7],\n",
        "    'text_prep__vectorizer__max_df': [.6]\n",
        "}]\n",
        "\n",
        "# Searching for best options\n",
        "grid_search_prep = GridSearchCV(e2e_pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search_prep.fit(X, y)\n",
        "\n",
        "print('Best params after a complete search:')\n",
        "print(grid_search_prep.best_params_)\n",
        "\n",
        "# Returning the best options\n",
        "vectorizer_max_features = grid_search_prep.best_params_['text_prep__vectorizer__max_features']\n",
        "vectorizer_min_df = grid_search_prep.best_params_['text_prep__vectorizer__min_df']\n",
        "vectorizer_max_df = grid_search_prep.best_params_['text_prep__vectorizer__max_df']\n",
        "\n",
        "# Updating the e2e pipeline with the best options found on search\n",
        "e2e_pipeline.named_steps['text_prep'].named_steps['vectorizer'].max_features = vectorizer_max_features\n",
        "e2e_pipeline.named_steps['text_prep'].named_steps['vectorizer'].min_df = vectorizer_min_df\n",
        "e2e_pipeline.named_steps['text_prep'].named_steps['vectorizer'].max_df = vectorizer_max_df\n",
        "\n",
        "# Fitting the model again\n",
        "e2e_pipeline.fit(X, y)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "--------- 4. MODELING  -----------\n",
        "    4.4 Final Model Performance\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "# Retrieving performance for te final model after hyperparam updating\n",
        "final_model = e2e_pipeline.named_steps['model']\n",
        "final_performance = cross_val_performance(final_model, X_prep, y, cv=5)\n",
        "final_performance = pd.concat([final_performance, performance])\n",
        "# final_performance.to_csv(METRICS_FILEPATH, index=False)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "--------- 4. MODELING  -----------\n",
        "      4.5 Saving pkl files\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "# Creating folders for saving pkl files (if not exists)\n",
        "if not os.path.exists('/content/models'):\n",
        "    os.makedirs('/content/models')\n",
        "if not os.path.exists('/content/pipelines'):\n",
        "    os.makedirs('/content/pipelines')\n",
        "if not os.path.exists('content/metrics'):\n",
        "    os.makedirs('content/metrics')\n",
        "\n",
        "# Saving pkl files\n",
        "dump(initial_prep_pipeline, os.path.join(PIPELINES_PATH, 'initial_prep_pipeline.pkl'))\n",
        "dump(text_prep_pipeline, os.path.join(PIPELINES_PATH, 'text_prep_pipeline.pkl'))\n",
        "dump(e2e_pipeline, os.path.join(PIPELINES_PATH, 'e2e_pipeline.pkl'))\n",
        "dump(final_model, os.path.join(MODELS_PATH, 'sentiment_clf_model.pkl'))"
      ],
      "metadata": {
        "id": "04PSO4oRCo7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "7J56TyYIV2pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_reviews_by_product(product_id):\n",
        "    \"\"\"\n",
        "    Analyzes reviews for a given product ID, returning a DataFrame with the count of\n",
        "    positive and negative reviews and the average review score.\n",
        "\n",
        "    Args:\n",
        "        product_id (str): The ID of the product to analyze.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with columns 'positive', 'negative', and 'average_review_score'.\n",
        "    \"\"\"\n",
        "\n",
        "    initial_prep_pipeline_path = 'pipelines/initial_prep_pipeline.pkl'\n",
        "    e2e_pipeline_path = 'pipelines/e2e_pipeline.pkl'\n",
        "\n",
        "    # Load the pipelines using joblib\n",
        "    initial_prep_pipeline = joblib.load(initial_prep_pipeline_path)\n",
        "    e2e_pipeline = joblib.load(e2e_pipeline_path)\n",
        "\n",
        "    # Select relevant columns\n",
        "    data = olist[['product_id', 'review_comment_message', 'review_score']]\n",
        "\n",
        "    data = initial_prep_pipeline.fit_transform(data)\n",
        "\n",
        "\n",
        "    # Filter by product ID\n",
        "    product_reviews = data[data['product_id'] == product_id]\n",
        "\n",
        "    # Handle missing review comments (assuming we want to keep the rows)\n",
        "    product_reviews['review_comment_message'] = product_reviews['review_comment_message'].fillna('')\n",
        "\n",
        "    # Preprocess review comments\n",
        "    product_reviews['review_comment_message'] = initial_prep_pipeline.transform(product_reviews['review_comment_message'])\n",
        "\n",
        "    # Predict sentiment for each review\n",
        "    predictions = e2e_pipeline.predict(product_reviews['review_comment_message'])\n",
        "\n",
        "    # Calculate average review score\n",
        "    average_score = product_reviews['review_score'].mean()\n",
        "\n",
        "    counts = pd.value_counts(predictions)\n",
        "    positive_count = counts.get(1, 0)\n",
        "    negative_count = counts.get(0, 0)\n",
        "\n",
        "    # Create DataFrame with results\n",
        "    result_df = pd.DataFrame({'review': product_reviews['review_comment_message'],\n",
        "                              'average_review_score': average_score})\n",
        "\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "ZVpnEzWoDn_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_id = '154e7e31ebfa092203795c972e5804a6'\n",
        "\n",
        "result = analyze_reviews_by_product(product_id)"
      ],
      "metadata": {
        "id": "WKKt09LtUSXw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}